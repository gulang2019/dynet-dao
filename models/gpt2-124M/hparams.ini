num-units=768
position-encoding=1
max-pos-seq-len=1024
num-heads=12
nlayers=12

ff-activation-type=1
emb-dropout-p=0
sublayer-dropout-p=0.1

attention-dropout-p=0.1
ff-dropout-p=0.1

minibatch-size=100
treport=512
dreport=20000
vocab=datasets/iwslt15/vocab.en
train=datasets/iwslt15/train.en.vb.capped
devel=datasets/iwslt15/tst2012.en.vb.capped
model-path=models/gpt2-124M
epochs=1
lr-eta=0.1
lr-patience=10
patience=20
lr-eta-decay=2