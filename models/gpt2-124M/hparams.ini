num-units=768
position-encoding=1
max-pos-seq-len=1024
max-seq-len=1024
num-heads=12
nlayers=12

ff-activation-type=1
emb-dropout-p=0
sublayer-dropout-p=0.1


minibatch-size=3
treport=100
dreport=5000
vocab=datasets/inst-tune/encoder.json
train=/home/siyuanch/ssd/workspace/GPT-4-LLM/data/alpaca_gpt4_data.json
epochs=1
sgd-trainer=4
lr-eta=0.001
lr-patience=10
patience=20
lr-eta-decay=2